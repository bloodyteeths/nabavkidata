# Semantic Search Tool Handler for RAG Query
# This code should be inserted into rag_query.py after the search_bid_documents handler

SEMANTIC_SEARCH_HANDLER = """
    elif tool_name == "semantic_search_documents":
        # Vector similarity search using pgvector and Gemini embeddings
        query_text = tool_args.get("query", "")
        if not query_text:
            return "–ù–µ –µ –¥–∞–¥–µ–Ω —Ç–µ–∫—Å—Ç –∑–∞ –ø—Ä–µ–±–∞—Ä—É–≤–∞—ö–µ."

        limit = tool_args.get("limit", 5)
        min_similarity = tool_args.get("min_similarity", 0.5)

        # Validate parameters
        if limit > 20:
            limit = 20
        if min_similarity < 0 or min_similarity > 1:
            min_similarity = 0.5

        try:
            # 1. Generate query embedding using Gemini
            logger.info(f"Generating embedding for query: {query_text[:100]}...")
            embedder = EmbeddingGenerator(api_key=os.getenv('GEMINI_API_KEY'))
            query_vector = await embedder.generate_embedding(query_text)

            # 2. Perform vector similarity search using pgvector
            # Use cosine distance operator <=> (1 - cosine similarity)
            vector_str = '[' + ','.join(map(str, query_vector)) + ']'

            # Search embeddings table with similarity threshold
            search_query = \"\"\"
                SELECT
                    e.embed_id,
                    e.chunk_text,
                    e.chunk_index,
                    e.tender_id,
                    e.doc_id,
                    e.metadata,
                    1 - (e.embedding <=> $1::vector) as similarity,
                    t.title as tender_title,
                    t.procuring_entity,
                    t.winner,
                    t.publication_date,
                    t.actual_value_mkd,
                    d.file_name,
                    d.doc_category
                FROM embeddings e
                LEFT JOIN tenders t ON e.tender_id = t.tender_id
                LEFT JOIN documents d ON e.doc_id = d.doc_id
                WHERE 1 - (e.embedding <=> $1::vector) >= $2
                ORDER BY e.embedding <=> $1::vector
                LIMIT $3
            \"\"\"

            rows = await conn.fetch(search_query, vector_str, min_similarity, limit)

            if not rows:
                return f"–ù–µ –Ω–∞—ò–¥–æ–≤ —Å–µ–º–∞–Ω—Ç–∏—á–∫–∏ —Å–ª–∏—á–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∑–∞: {query_text}\\n(–ú–æ–∂–µ–±–∏ –ø—Ä–æ–±–∞—ò—Ç–µ —Å–æ –ø–æ–º–∞–ª min_similarity –∏–ª–∏ –∫–æ—Ä–∏—Å—Ç–µ—Ç–µ keyword search)"

            # 3. Format results with context
            result_parts = [
                f"üîç –°–µ–º–∞–Ω—Ç–∏—á–∫–æ –ø—Ä–µ–±–∞—Ä—É–≤–∞—ö–µ: {query_text}",
                f"–ù–∞—ò–¥–æ–≤ {len(rows)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ (—Å–ª–∏—á–Ω–æ—Å—Ç >= {min_similarity:.0%}):\\n"
            ]

            for i, row in enumerate(rows, 1):
                similarity_pct = row['similarity'] * 100
                chunk_text = row['chunk_text'][:1500] if row['chunk_text'] else "–ù–µ–º–∞ —Å–æ–¥—Ä–∂–∏–Ω–∞"

                result_parts.append(f"\\n{'='*60}")
                result_parts.append(f"–†–µ–∑—É–ª—Ç–∞—Ç #{i} (–°–ª–∏—á–Ω–æ—Å—Ç: {similarity_pct:.1f}%)")
                result_parts.append(f"{'='*60}")

                # Tender info (if available)
                if row['tender_title']:
                    result_parts.append(f"**–¢–µ–Ω–¥–µ—Ä:** {row['tender_title']}")
                if row['procuring_entity']:
                    result_parts.append(f"**–ù–∞–±–∞–≤—É–≤–∞—á:** {row['procuring_entity']}")
                if row['winner']:
                    result_parts.append(f"**–ü–æ–±–µ–¥–Ω–∏–∫:** {row['winner']}")
                if row['publication_date']:
                    result_parts.append(f"**–î–∞—Ç—É–º:** {row['publication_date']}")
                if row['actual_value_mkd']:
                    result_parts.append(f"**–í—Ä–µ–¥–Ω–æ—Å—Ç:** {row['actual_value_mkd']:,.0f} –ú–ö–î")

                # Document info
                if row['file_name']:
                    result_parts.append(f"**–î–æ–∫—É–º–µ–Ω—Ç:** {row['file_name']} ({row['doc_category'] or 'N/A'})")

                # Metadata
                if row['metadata']:
                    metadata = row['metadata']
                    if isinstance(metadata, str):
                        import json
                        try:
                            metadata = json.loads(metadata)
                        except:
                            pass
                    if isinstance(metadata, dict) and metadata:
                        meta_str = ", ".join(f"{k}: {v}" for k, v in metadata.items() if v)
                        if meta_str:
                            result_parts.append(f"**–ú–µ—Ç–∞–ø–æ–¥–∞—Ç–æ—Ü–∏:** {meta_str}")

                # Chunk content
                result_parts.append(f"\\n**–°–æ–¥—Ä–∂–∏–Ω–∞:**")
                result_parts.append(chunk_text)

            # Summary statistics
            avg_similarity = sum(r['similarity'] for r in rows) / len(rows)
            result_parts.append(f"\\n{'='*60}")
            result_parts.append(f"üìä –ü—Ä–æ—Å–µ—á–Ω–∞ —Å–ª–∏—á–Ω–æ—Å—Ç: {avg_similarity*100:.1f}%")
            result_parts.append(f"üí° –°–æ–≤–µ—Ç: –ó–∞ –ø–æ–¥–æ–±—Ä–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏, –∫–æ—Ä–∏—Å—Ç–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∏ —Ç–µ—Ä–º–∏–Ω–∏ –∏ –æ–ø–∏—à–∏ —à—Ç–æ –±–∞—Ä–∞—à.")

            return "\\n".join(result_parts)

        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏—á–∫–æ –ø—Ä–µ–±–∞—Ä—É–≤–∞—ö–µ: {str(e)}\\n–ü—Ä–æ–±–∞—ò—Ç–µ —Å–æ keyword search (search_bid_documents) –∫–∞–∫–æ –∞–ª—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞."
"""

# Instructions for insertion:
# 1. Find line: elif tool_name == "web_search_procurement":
# 2. Insert the SEMANTIC_SEARCH_HANDLER code BEFORE that line
# 3. Make sure indentation matches the surrounding elif blocks
