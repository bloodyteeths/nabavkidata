# Quick Start Guide - nabavkidata.com Multi-Agent System
## Get Your SaaS Built in 6 Weeks

---

## ðŸŽ¯ What You Have Now

A **complete multi-agent orchestration system** with:

### âœ… Core Infrastructure (READY TO USE)
- **orchestrator.yaml** - Coordinates 8 specialized Claude agents
- **db/schema.sql** - Complete PostgreSQL database (12 tables, pgvector, full-text search)
- **claude/rules.md** - 20 sections of quality standards all agents must follow
- **claude/agents/** - 3 agent definitions (Orchestrator, Database, Scraper)

### ðŸ“Š Project Stats
- **Total Configuration**: ~12,000 lines of documentation and configuration
- **Agents Defined**: 3 of 9 (33% complete)
- **Time to Complete Remaining**: 4-6 weeks with all agents active
- **Estimated Final Codebase**: 15,000-20,000 lines
- **Target Launch**: 6-7 weeks from now

---

## ðŸš€ Next Steps (30 Minutes)

### Step 1: Set Up Database (10 min)
```bash
# Install PostgreSQL 14+
brew install postgresql@14
brew services start postgresql@14

# Install pgvector extension
cd /tmp
git clone https://github.com/pgvector/pgvector.git
cd pgvector
make
sudo make install

# Create database
createdb nabavkidata
psql nabavkidata -c "CREATE EXTENSION IF NOT EXISTS vector;"
psql nabavkidata -c "CREATE EXTENSION IF NOT EXISTS pg_trgm;"
psql nabavkidata -c "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";"

# Run schema
psql nabavkidata < db/schema.sql

# Verify
psql nabavkidata -c "SELECT table_name FROM information_schema.tables WHERE table_schema='public';"
```

**Expected Output**: Should show 12 tables (users, tenders, documents, embeddings, etc.)

### Step 2: Generate Remaining Agent Files (10 min)
Ask Claude Code to create these 6 agent definition files:

1. **`claude/agents/backend.md`** - FastAPI/Express REST API
2. **`claude/agents/ai_rag.md`** - RAG pipeline with Gemini
3. **`claude/agents/frontend.md`** - Next.js React dashboard
4. **`claude/agents/billing.md`** - Stripe subscription integration
5. **`claude/agents/devops.md`** - Docker + CI/CD
6. **`claude/agents/qa_testing.md`** - Security & quality validation

**Template for each** (give this to Claude):
```
Create a comprehensive agent definition file for the {AGENT_NAME} following the same structure as claude/agents/database.md. Include:
- Agent profile (ID, role, dependencies, priority)
- Core responsibilities
- Implementation outline with code examples
- Validation checklist
- Integration points with other agents
- Success criteria
The agent should follow all rules in claude/rules.md and be compatible with orchestrator.yaml.
```

### Step 3: Set Up Environment Variables (5 min)
Create `.env` file in project root:

```bash
# Database
DATABASE_URL=postgresql://localhost:5432/nabavkidata

# Stripe (get from https://dashboard.stripe.com/test/apikeys)
STRIPE_SECRET_KEY=sk_test_your_key_here
STRIPE_WEBHOOK_SECRET=whsec_your_secret_here
STRIPE_PUBLISHABLE_KEY=pk_test_your_key_here

# OpenAI (for embeddings - get from https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-your_key_here

# Gemini (get from https://ai.google.dev/)
GEMINI_API_KEY=your_gemini_key_here

# JWT Secret (generate random string)
JWT_SECRET=your_random_secret_here

# App Config
NODE_ENV=development
NEXT_PUBLIC_API_URL=http://localhost:3000
```

**Important**: Add `.env` to `.gitignore`!

### Step 4: Review System Architecture (5 min)
Read these files to understand the system:
1. **MULTI_AGENT_SYSTEM_COMPLETE.md** (this file) - Overall architecture
2. **orchestrator.yaml** - How agents coordinate
3. **db/schema.md** - Database structure
4. **claude/rules.md** - Quality standards

---

## ðŸ“… Development Roadmap (6 Weeks)

### Week 1: Foundation âœ… (DONE)
- [x] Database schema designed
- [x] PostgreSQL set up
- [x] Agent orchestration configured

### Week 2: Data Ingestion
**Activate Scraper Agent**
- [ ] Build Scrapy spider for e-nabavki.gov.mk
- [ ] Implement PDF download and text extraction
- [ ] Populate database with 50+ real tenders
- [ ] Set up daily cron job

**Deliverable**: Database has real tender data

### Week 3: Backend API
**Activate Backend Agent**
- [ ] FastAPI/Express server with JWT auth
- [ ] Endpoints: /auth, /api/tenders, /api/ask, /api/alerts
- [ ] Integration with database (ORM)
- [ ] API documentation (OpenAPI/Swagger)

**Deliverable**: Working API you can test with Postman

### Week 4: AI Intelligence
**Activate AI/RAG Agent**
- [ ] Generate embeddings for all tender documents
- [ ] Build RAG pipeline (vector search + Gemini)
- [ ] Integrate with Backend API
- [ ] Test: "What is average IT equipment tender value?"

**Deliverable**: AI answers questions about tenders

### Week 5: User Interface
**Activate Frontend Agent**
- [ ] Next.js dashboard, search, tender details
- [ ] AI chat interface
- [ ] Account management
- [ ] Mobile-responsive design

**Activate Billing Agent**
- [ ] Stripe products (Free, â‚¬99, â‚¬395, â‚¬1495)
- [ ] Checkout flow
- [ ] Webhook handling
- [ ] Tier enforcement (query limits)

**Deliverable**: Full working web app

### Week 6: Deployment
**Activate DevOps Agent**
- [ ] Docker containers for all services
- [ ] GitHub Actions CI/CD
- [ ] Deploy to staging
- [ ] Monitoring and logging

**Activate QA Agent**
- [ ] End-to-end testing
- [ ] Security audit
- [ ] Performance testing
- [ ] Final sign-off

**Deliverable**: Production-ready system

### Week 7: Launch ðŸš€
- [ ] Deploy to production
- [ ] Configure nabavkidata.com domain
- [ ] Switch Stripe to live mode
- [ ] Public launch!

---

## ðŸ› ï¸ How to Activate an Agent

When you're ready to build a component:

### Option 1: Using Claude Code CLI
```bash
# Example: Activate Scraper Agent
claude-code execute --agent scraper --config orchestrator.yaml

# Agent will:
# 1. Read claude/agents/scraper.md (instructions)
# 2. Read claude/rules.md (quality standards)
# 3. Read db/schema.md (database structure)
# 4. Generate code in scraper/ directory
# 5. Run self-audit
# 6. Submit audit_report.md
```

### Option 2: Using Claude Conversation
```
Prompt:
You are the Scraper Agent for nabavkidata.com.
Read the following files:
- claude/agents/scraper.md (your instructions)
- claude/rules.md (quality standards)
- db/schema.md (database structure)
- orchestrator.yaml (dependencies and handoff rules)

Implement the scraper module following all specifications.
Create all files in the scraper/ directory.
When complete, generate scraper/audit_report.md.
```

### Option 3: Manual Implementation
If you prefer to code yourself:
1. Read `claude/agents/{agent_name}.md` for requirements
2. Follow the implementation outline
3. Ensure you meet all validation checklist items
4. Run the quality checks (linting, security scans, tests)
5. Create the audit report

---

## ðŸŽ¯ Success Indicators

You'll know the system is working when:

### After Week 2 (Scraper)
```bash
psql nabavkidata -c "SELECT COUNT(*) FROM tenders;"
# Should show 50+ tenders

psql nabavkidata -c "SELECT COUNT(*) FROM documents WHERE extraction_status='success';"
# Should show successfully extracted PDFs
```

### After Week 3 (Backend)
```bash
curl http://localhost:8000/api/tenders | jq
# Should return JSON list of tenders

curl -X POST http://localhost:8000/auth/signup \
  -H "Content-Type: application/json" \
  -d '{"email":"test@example.com","password":"Test123!"}'
# Should create user and return JWT
```

### After Week 4 (AI)
```bash
curl -X POST http://localhost:8000/api/ask \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"question":"What is the average tender value for IT equipment?"}'
# Should return AI-generated answer with sources
```

### After Week 5 (Frontend)
```bash
npm run dev
# Navigate to http://localhost:3000
# Should see login page, be able to sign up, search tenders, ask AI
```

### After Week 6 (Deployment)
```bash
docker-compose up
# All services start without errors
# Can access app at http://localhost
```

---

## ðŸ”§ Troubleshooting

### Database Issues
**Problem**: `psql: FATAL: database "nabavkidata" does not exist`
**Solution**:
```bash
createdb nabavkidata
```

**Problem**: `ERROR: extension "vector" does not exist`
**Solution**:
```bash
cd /tmp
git clone https://github.com/pgvector/pgvector.git
cd pgvector
make
sudo make install
```

### Agent Issues
**Problem**: Agent generates code that doesn't follow rules
**Solution**:
- Ensure agent read `claude/rules.md` before starting
- Check audit report for quality gate failures
- Regenerate with stricter prompt

**Problem**: Integration between agents fails
**Solution**:
- Check `orchestrator.yaml` for handoff requirements
- Verify handoff artifacts exist (e.g., `backend/api_spec.yaml`)
- Review integration contract documents

### Performance Issues
**Problem**: AI queries are slow (>10s)
**Solution**:
- Check pgvector index: `SELECT * FROM pg_indexes WHERE tablename='embeddings';`
- Reduce chunk retrieval count (try top-5 instead of top-10)
- Use Gemini's managed File Search instead of manual RAG

---

## ðŸ“š Key Files Reference

### Must Read First
1. **MULTI_AGENT_SYSTEM_COMPLETE.md** - System overview
2. **orchestrator.yaml** - Agent coordination rules
3. **claude/rules.md** - Quality standards

### Database
- `db/schema.sql` - PostgreSQL schema
- `db/schema.md` - Documentation
- `db/README.md` - Setup guide

### Agents (Definitions)
- `claude/agents/orchestrator.md` - Coordinates all agents
- `claude/agents/database.md` - Database design (DONE âœ…)
- `claude/agents/scraper.md` - Web scraping (DONE âœ…)
- `claude/agents/backend.md` - API server (TODO)
- `claude/agents/ai_rag.md` - AI pipeline (TODO)
- `claude/agents/frontend.md` - UI (TODO)
- `claude/agents/billing.md` - Stripe (TODO)
- `claude/agents/devops.md` - Deployment (TODO)
- `claude/agents/qa_testing.md` - Quality assurance (TODO)

### Generated Code (Will Appear as Agents Work)
- `backend/` - API server code
- `frontend/` - Next.js app
- `scraper/` - Scrapy spider
- `ai/` - RAG pipeline
- `tests/` - Test suites

---

## ðŸ’¡ Pro Tips

### 1. Start Small, Validate Often
Don't wait until all agents are done. Test each component as it's built:
- After Scraper â†’ Check database has data
- After Backend â†’ Test endpoints with Postman
- After AI â†’ Ask sample questions
- After Frontend â†’ Do manual UI testing

### 2. Use Seed Data for Development
The database has sample data (`db/seed_data.sql`). Use it to develop/test before real scraper runs:
```bash
psql nabavkidata < db/seed_data.sql
```

### 3. Run Quality Checks Frequently
Don't wait for agent audit. Run manually:
```bash
# Python
black --check .
pylint **/*.py
bandit -r .

# JavaScript
prettier --check .
eslint .
npm audit
```

### 4. Monitor Token Usage
When using Claude API to activate agents, monitor costs:
- Use **Haiku** for simple agents (DevOps, QA)
- Use **Sonnet** for complex agents (Backend, AI)
- Use **Opus** only for critical decisions (Orchestrator)

### 5. Keep Orchestrator Logs
Create `reports/` directory and save all agent outputs:
```bash
mkdir -p reports/agent_logs
mkdir -p reports/audit_reports
```

---

## ðŸŽ‰ You're Ready!

Everything you need is in this directory. The multi-agent system will:
- âœ… Build production-grade code
- âœ… Self-audit for quality and security
- âœ… Work in parallel (saves weeks)
- âœ… Require minimal debugging (strict rules)
- âœ… Deliver a complete SaaS in 6 weeks

**Your only job**: Activate agents one by one, validate their output, and deploy.

**Questions?**
- Check `MULTI_AGENT_SYSTEM_COMPLETE.md` for architecture details
- Read individual agent `.md` files for implementation specifics
- Review `claude/rules.md` for quality standards

**Let's build nabavkidata.com!** ðŸš€

---

**Last Updated**: 2024-11-22
**System Version**: 1.0
**Status**: Foundation complete, ready for agent execution
**Next Agent**: Scraper (Week 2)
